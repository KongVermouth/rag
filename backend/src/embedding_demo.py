#!/usr/bin/env python3
import argparse
import sys
from pathlib import Path
from typing import List, Union
import numpy as np
from scipy.spatial.distance import cosine
from transformers import AutoTokenizer, AutoModel
import torch


current_dir = Path(__file__).resolve().parent
model_path = (current_dir.parent / "models" / "Qwen" / "Qwen3-Embedding-0___6B").resolve()
print(f"è§£æåçš„æ¨¡å‹ç»å¯¹è·¯å¾„: {model_path}")


class QwenEmbeddingModel:
    """Qwen3-Embedding æ¨¡å‹å°è£…ç±»"""
    
    def __init__(self, model_path: Union[str, Path], device: str = "auto"):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ('cpu', 'cuda', 'auto')
        """
        self.model_path = Path(model_path)
        
        # å†æ¬¡ç¡®ä¿è·¯å¾„å¯¹è±¡å­˜åœ¨
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.model_path}")

        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½ tokenizer å’Œ model
        # --- ä¿®æ”¹å¼€å§‹ ---
        # å…³é”®ä¿®æ”¹ï¼šæ·»åŠ  local_files_only=True
        # è¿™ä¼šå¼ºåˆ¶ Transformers åªä»æœ¬åœ°åŠ è½½ï¼Œå¿½ç•¥ HF Hub çš„ ID éªŒè¯é€»è¾‘
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(self.model_path),
            trust_remote_code=True,
            local_files_only=True
        )
        self.model = AutoModel.from_pretrained(
            str(self.model_path),
            trust_remote_code=True,
            local_files_only=True
        ).to(self.device)
        # --- ä¿®æ”¹ç»“æŸ ---
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def encode(
        self, 
        texts: Union[str, List[str]], 
        normalize: bool = True,
        max_length: int = 512
    ) -> np.ndarray:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦å¯¹å‘é‡è¿›è¡ŒL2å½’ä¸€åŒ–
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            
        Returns:
            æ–‡æœ¬åµŒå…¥å‘é‡ï¼Œshape: (n_texts, embedding_dim)
        """
        if isinstance(texts, str):
            texts = [texts]
        
        # Tokenize
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        ).to(self.device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            # Qwen embedding é€šå¸¸å–æœ€åä¸€å±‚éšè—çŠ¶æ€çš„ [CLS] token æˆ–å¹³å‡æ± åŒ–
            embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        
        # è½¬ä¸º numpy
        embeddings = embeddings.cpu().numpy()
        
        # L2 å½’ä¸€åŒ–
        if normalize:
            embeddings = embeddings / np.linalg.norm(
                embeddings, axis=1, keepdims=True
            )
        
        return embeddings
    
    def compute_similarity(
        self, 
        text1: Union[str, List[str]], 
        text2: Union[str, List[str]]
    ) -> np.ndarray:
        """
        è®¡ç®—æ–‡æœ¬é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            text1: ç¬¬ä¸€ç»„æ–‡æœ¬
            text2: ç¬¬äºŒç»„æ–‡æœ¬
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        emb1 = self.encode(text1, normalize=True)
        emb2 = self.encode(text2, normalize=True)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦ = 1 - ä½™å¼¦è·ç¦»
        similarities = []
        for e1 in emb1:
            row = []
            for e2 in emb2:
                sim = 1 - cosine(e1, e2)
                row.append(sim)
            similarities.append(row)
        
        return np.array(similarities)


def print_separator(char: str = "=", length: int = 60):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def test_basic_embedding(model: QwenEmbeddingModel):
    """æµ‹è¯•1: åŸºæœ¬æ–‡æœ¬åµŒå…¥"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: åŸºæœ¬æ–‡æœ¬åµŒå…¥")
    print("="*60)
    
    texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "Pythonæ˜¯ä¸€é—¨ç¼–ç¨‹è¯­è¨€",
        "æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é¢†åŸŸ"
    ]
    
    embeddings = model.encode(texts)
    
    print(f"\nè¾“å…¥æ–‡æœ¬æ•°: {len(texts)}")
    print(f"åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
    print(f"åµŒå…¥å‘é‡å½¢çŠ¶: {embeddings.shape}")
    
    print("\nå„æ–‡æœ¬åµŒå…¥å‘é‡å‰5ä¸ªç»´åº¦çš„å€¼:")
    for i, (text, emb) in enumerate(zip(texts, embeddings), 1):
        print(f"\n{i}. {text}")
        print(f"   å‰5ç»´: {emb[:5]}")
        print(f"   èŒƒæ•°: {np.linalg.norm(emb):.6f}")


def test_similarity(model: QwenEmbeddingModel):
    """æµ‹è¯•2: æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—")
    print("="*60)
    
    query = "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ "
    candidates = [
        "æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
        "ä»Šå¤©å¤©æ°”çœŸå¥½",
        "æ·±åº¦å­¦ä¹ å…¥é—¨æ•™ç¨‹",
        "æˆ‘å–œæ¬¢åƒè‹¹æœ",
        "Pythonæœºå™¨å­¦ä¹ å®æˆ˜æŒ‡å—"
    ]
    
    print(f"\næŸ¥è¯¢æ–‡æœ¬: ã€Œ{query}ã€")
    print("\nå€™é€‰æ–‡æœ¬:")
    for i, text in enumerate(candidates, 1):
        print(f"  {i}. {text}")
    
    similarities = model.compute_similarity(query, candidates)
    
    print("\nç›¸ä¼¼åº¦å¾—åˆ†:")
    sorted_indices = np.argsort(similarities[0])[::-1]  # é™åº
    
    for rank, idx in enumerate(sorted_indices, 1):
        sim = similarities[0][idx]
        print(f"  {rank}. [{sim:.4f}] {candidates[idx]}")


def test_batch_encoding(model: QwenEmbeddingModel):
    """æµ‹è¯•3: æ‰¹é‡ç¼–ç æ•ˆç‡"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: æ‰¹é‡ç¼–ç æ•ˆç‡å¯¹æ¯”")
    print("="*60)
    
    import time
    
    test_texts = [
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦é¢†åŸŸ",
        "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å›¾åƒ",
        "å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æœºåˆ¶è®­ç»ƒæ™ºèƒ½ä½“",
        "ç”Ÿæˆå¼AIå¯ä»¥åˆ›ä½œæ–°çš„å†…å®¹",
        "å¤§è¯­è¨€æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„ç†è§£èƒ½åŠ›"
    ]
    
    # å•æ¡å¤„ç†
    start_time = time.time()
    single_results = [model.encode(text) for text in test_texts]
    single_time = time.time() - start_time
    
    # æ‰¹é‡å¤„ç†
    start_time = time.time()
    batch_results = model.encode(test_texts)
    batch_time = time.time() - start_time
    
    print(f"\næ–‡æœ¬æ•°é‡: {len(test_texts)}")
    print(f"å•æ¡å¤„ç†è€—æ—¶: {single_time:.3f}s")
    print(f"æ‰¹é‡å¤„ç†è€—æ—¶: {batch_time:.3f}s")
    print(f"æ‰¹é‡åŠ é€Ÿæ¯”: {single_time / batch_time:.2f}x")
    
    # éªŒè¯ç»“æœä¸€è‡´
    single_concat = np.vstack(single_results)
    max_diff = np.max(np.abs(single_concat - batch_results))
    print(f"å•æ¡ä¸æ‰¹é‡ç»“æœæœ€å¤§å·®å¼‚: {max_diff:.10f}")


def test_semantic_search(model: QwenEmbeddingModel):
    """æµ‹è¯•4: è¯­ä¹‰æœç´¢æ¼”ç¤º"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: è¯­ä¹‰æœç´¢æ¼”ç¤º")
    print("="*60)
    
    # æ„å»ºæ–‡æ¡£åº“
    documents = [
        {"id": 1, "content": "Transformeræ˜¯å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„", "category": "æ·±åº¦å­¦ä¹ "},
        {"id": 2, "content": "å·ç§¯ç¥ç»ç½‘ç»œCNNä¸»è¦ç”¨äºå›¾åƒå¤„ç†ä»»åŠ¡", "category": "è®¡ç®—æœºè§†è§‰"},
        {"id": 3, "content": "å¾ªç¯ç¥ç»ç½‘ç»œRNNé€‚åˆå¤„ç†æ—¶åºæ•°æ®", "category": "æ·±åº¦å­¦ä¹ "},
        {"id": 4, "content": "BERTæ˜¯åŒå‘ç¼–ç çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹", "category": "NLP"},
        {"id": 5, "content": "ResNetè§£å†³äº†æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜", "category": "è®¡ç®—æœºè§†è§‰"},
        {"id": 6, "content": "Attentionæœºåˆ¶è®©æ¨¡å‹èƒ½å…³æ³¨é‡è¦ä¿¡æ¯", "category": "æ·±åº¦å­¦ä¹ "},
        {"id": 7, "content": "GPTç³»åˆ—æ˜¯ç”Ÿæˆå¼é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹", "category": "NLP"},
        {"id": 8, "content": "YOLOæ˜¯ä¸€ç§å®æ—¶çš„ç›®æ ‡æ£€æµ‹ç®—æ³•", "category": "è®¡ç®—æœºè§†è§‰"},
    ]
    
    # é¢„ç¼–ç æ–‡æ¡£
    doc_texts = [doc["content"] for doc in documents]
    doc_embeddings = model.encode(doc_texts)
    
    # æŸ¥è¯¢
    queries = [
        "ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶",
        "å›¾åƒè¯†åˆ«ç”¨ä»€ä¹ˆæ¨¡å‹",
        "æ–‡æœ¬ç”Ÿæˆæœ‰å“ªäº›æ¨¡å‹"
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: ã€Œ{query}ã€")
        query_emb = model.encode(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(doc_embeddings, query_emb.T).flatten()
        
        # æ˜¾ç¤ºTop-3
        top_k = 3
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        print(f"\nTop-{top_k} åŒ¹é…ç»“æœ:")
        for rank, idx in enumerate(top_indices, 1):
            doc = documents[idx]
            sim = similarities[idx]
            print(f"  {rank}. [{sim:.4f}] ({doc['category']}) {doc['content']}")


def interactive_mode(model: QwenEmbeddingModel):
    """äº¤äº’å¼æ¨¡å¼"""
    print("\n" + "="*60)
    print("äº¤äº’å¼æ¨¡å¼")
    print("="*60)
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")
    
    while True:
        try:
            text = input("è¯·è¾“å…¥æ–‡æœ¬: ").strip()
            
            if text.lower() in ('quit', 'exit', 'q'):
                print("å†è§ï¼")
                break
            
            if not text:
                continue
            
            embedding = model.encode(text)
            print(f"\nåµŒå…¥å‘é‡å½¢çŠ¶: {embedding.shape}")
            print(f"å‰10ä¸ªç»´åº¦çš„å€¼:\n{embedding[0][:10]}")
            print(f"å‘é‡èŒƒæ•°: {np.linalg.norm(embedding[0]):.6f}\n")
            
        except KeyboardInterrupt:
            print("\n\nå†è§ï¼")
            break


def main(argv: list[str] | None = None) -> int:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Qwen3-Embedding æ¨¡å‹åŠ è½½ä¸æµ‹è¯•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["cpu", "cuda", "auto"],
        help="è¿è¡Œè®¾å¤‡"
    )
    parser.add_argument(
        "--test",
        type=str,
        default="all",
        choices=["basic", "similarity", "batch", "search", "all", "none"],
        help="æŒ‡å®šè¿è¡Œçš„æµ‹è¯•"
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="å¯ç”¨äº¤äº’å¼æ¨¡å¼"
    )
    
    args = parser.parse_args(argv)
    
    try:
        # åŠ è½½æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ model_path (å·²ç»æ˜¯ Path å¯¹è±¡)
        model = QwenEmbeddingModel(model_path, args.device)
        
        # è¿è¡Œæµ‹è¯•
        if args.test in ("basic", "all"):
            test_basic_embedding(model)
        
        if args.test in ("similarity", "all"):
            test_similarity(model)
        
        if args.test in ("batch", "all"):
            test_batch_encoding(model)
        
        if args.test in ("search", "all"):
            test_semantic_search(model)
        
        # äº¤äº’å¼æ¨¡å¼
        if args.interactive:
            interactive_mode(model)
        
        return 0
        
    except Exception as e:
        print(f"\né”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
